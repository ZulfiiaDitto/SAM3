{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZulfiiaDitto/SAM3/blob/main/SAM3_text_prompting_medical_microscopy_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure your API keys\n",
        "\n",
        "To pull Segment Anything 3 weights, you need a HuggingFace Access Token with approved access to the SAM3 checkpoints.\n",
        "Steps you need to do:\n",
        "\n",
        "1. Request access to the SAM3 from official Hugging Face [repo](https://github.com/facebookresearch/sam3).\n",
        "2. Open your HuggingFace Settings page. Click Access Tokens then New Token to generate a new token.\n",
        "3. Go to the left pane in Colab and click on Secrets (ðŸ”‘). Store your HuggingFace Access Token under the name `HF_TOKEN`.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wTtxQINkjhKY"
      },
      "id": "wTtxQINkjhKY"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")"
      ],
      "metadata": {
        "id": "7FCPftOXHHrN"
      },
      "id": "7FCPftOXHHrN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check GPU availability\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `T4 GPU`, and then click `Save`."
      ],
      "metadata": {
        "id": "ncCj5PUgxVV6"
      },
      "id": "ncCj5PUgxVV6"
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "ILPFDHXR8Cfh",
        "collapsed": true
      },
      "id": "ILPFDHXR8Cfh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)\n",
        "print(\"CUDA is available:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "Vw3admXIqMlC"
      },
      "id": "Vw3admXIqMlC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install SAM 3 and extra dependencies"
      ],
      "metadata": {
        "id": "bkMO3Xc-yTKi"
      },
      "id": "bkMO3Xc-yTKi"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/sam3.git\n",
        "%cd sam3\n",
        "!pip install -e \".[notebooks]\"\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "qq18j8epn6uG",
        "collapsed": true
      },
      "id": "qq18j8epn6uG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q supervision jupyter_bbox_widget"
      ],
      "metadata": {
        "id": "eM_DGtMk_gFM"
      },
      "id": "eM_DGtMk_gFM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load SAM3 Image Predictor\n"
      ],
      "metadata": {
        "id": "on3IzFeee1R4"
      },
      "id": "on3IzFeee1R4"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "\n",
        "if torch.cuda.get_device_properties(0).major >= 8:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True"
      ],
      "metadata": {
        "id": "IjIVRbthCMRY"
      },
      "id": "IjIVRbthCMRY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sam3.model_builder import build_sam3_image_model\n",
        "from sam3.model.sam3_image_processor import Sam3Processor\n",
        "\n",
        "model = build_sam3_image_model()\n",
        "processor = Sam3Processor(model, confidence_threshold=0.3)"
      ],
      "metadata": {
        "id": "zS_imm_KJ4nx"
      },
      "id": "zS_imm_KJ4nx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few utils to parse and visualize the result"
      ],
      "metadata": {
        "id": "oMs6cjnYdyfn"
      },
      "id": "oMs6cjnYdyfn"
    },
    {
      "cell_type": "code",
      "source": [
        "import supervision as sv\n",
        "\n",
        "def from_sam(sam_result: dict) -> sv.Detections:\n",
        "    xyxy = sam_result[\"boxes\"].to(torch.float32).cpu().numpy()\n",
        "    confidence = sam_result[\"scores\"].to(torch.float32).cpu().numpy()\n",
        "\n",
        "    mask = sam_result[\"masks\"].to(torch.bool)\n",
        "    mask = mask.reshape(mask.shape[0], mask.shape[2], mask.shape[3]).cpu().numpy()\n",
        "\n",
        "    return sv.Detections(\n",
        "        xyxy=xyxy,\n",
        "        confidence=confidence,\n",
        "        mask=mask\n",
        "    )"
      ],
      "metadata": {
        "id": "C48TFYoNFHKg"
      },
      "id": "C48TFYoNFHKg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import supervision as sv\n",
        "from PIL import Image\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "COLOR = sv.ColorPalette.from_hex([\n",
        "    \"#ffff00\", \"#ff9b00\", \"#ff8080\", \"#ff66b2\", \"#ff66ff\", \"#b266ff\",\n",
        "    \"#9999ff\", \"#3399ff\", \"#66ffff\", \"#33ff99\", \"#66ff66\", \"#99ff00\"\n",
        "])\n",
        "\n",
        "\n",
        "def annotate(image: Image.Image, detections: sv.Detections, label: Optional[str] = None) -> Image.Image:\n",
        "    text_scale = sv.calculate_optimal_text_scale(resolution_wh=image.size)\n",
        "\n",
        "    mask_annotator = sv.MaskAnnotator(\n",
        "        color=COLOR,\n",
        "        color_lookup=sv.ColorLookup.INDEX,\n",
        "        opacity=0.6\n",
        "    )\n",
        "    box_annotator = sv.BoxAnnotator(\n",
        "        color=COLOR,\n",
        "        color_lookup=sv.ColorLookup.INDEX,\n",
        "        thickness=1\n",
        "    )\n",
        "    label_annotator = sv.LabelAnnotator(\n",
        "        color=COLOR,\n",
        "        color_lookup=sv.ColorLookup.INDEX,\n",
        "        text_scale=0.4,\n",
        "        text_padding=5,\n",
        "        text_color=sv.Color.BLACK,\n",
        "        text_thickness=1\n",
        "    )\n",
        "\n",
        "    annotated_image = image.copy()\n",
        "    annotated_image = mask_annotator.annotate(annotated_image, detections)\n",
        "    annotated_image = box_annotator.annotate(annotated_image, detections)\n",
        "\n",
        "    if label:\n",
        "        labels = [\n",
        "            f\"{label} {confidence:.2f}\"\n",
        "            for confidence in detections.confidence\n",
        "        ]\n",
        "        annotated_image = label_annotator.annotate(annotated_image, detections, labels)\n",
        "\n",
        "    return annotated_image"
      ],
      "metadata": {
        "id": "pzWvGh4bFRL0"
      },
      "id": "pzWvGh4bFRL0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAM 3 text prompt"
      ],
      "metadata": {
        "id": "JmI33i-OOsoT"
      },
      "id": "JmI33i-OOsoT"
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "PROMPT = \"cells\"\n",
        "IMAGE_PATH = '/content/63636-Platelet satellitism.Jpeg'\n",
        "\n",
        "image = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
        "inference_state = processor.set_image(image)\n",
        "inference_state = processor.set_text_prompt(state=inference_state, prompt=PROMPT)\n",
        "\n",
        "detections = from_sam(sam_result=inference_state)\n",
        "detections = detections[detections.confidence > 0.5]\n",
        "\n",
        "print(f\"There are {len(detections)} {PROMPT} objects detected in the image.\\n\")\n",
        "annotate(image, detections, label=PROMPT)"
      ],
      "metadata": {
        "id": "F4dDUDFDktqu"
      },
      "id": "F4dDUDFDktqu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAM 3 box prompt"
      ],
      "metadata": {
        "id": "bef5BhqdRgwH"
      },
      "id": "bef5BhqdRgwH"
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "OBJECTS = ['positive', 'negative']\n",
        "\n",
        "def encode_image_pillow(image: Image.Image) -> str:\n",
        "    buffer = BytesIO()\n",
        "    image.save(buffer, format=\"JPEG\")\n",
        "    image_bytes = buffer.getvalue()\n",
        "    encoded = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
        "    return \"data:image/jpeg;base64,\" + encoded"
      ],
      "metadata": {
        "id": "geCOmwsMdVAo"
      },
      "id": "geCOmwsMdVAo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When prompting SAM 3 with bounding boxes, the model expects boxes in the `xcycwh` format (`x_center`, `y_center`, `width`, `height`), and the coordinates must be normalized. The code below handles the conversion to this format."
      ],
      "metadata": {
        "id": "kTXyfJDerQ9t"
      },
      "id": "kTXyfJDerQ9t"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_normalized_boxes(bboxes, label, resolution_wh):\n",
        "    width, height = resolution_wh\n",
        "    boxes = [\n",
        "        [b[\"x\"] + b[\"width\"] / 2, b[\"y\"] + b[\"height\"] / 2, b[\"width\"], b[\"height\"]]\n",
        "        for b in bboxes\n",
        "        if b[\"label\"] == label\n",
        "    ]\n",
        "    if not boxes:\n",
        "        return np.empty((0, 4), dtype=np.float32)\n",
        "    scale = np.array([width, height, width, height], dtype=np.float32).reshape(1,-1)\n",
        "    return np.array(boxes, dtype=np.float32) / scale"
      ],
      "metadata": {
        "id": "iq4R9ZG-ngKf"
      },
      "id": "iq4R9ZG-ngKf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from jupyter_bbox_widget import BBoxWidget\n",
        "\n",
        "image_path = '/content/63636-Platelet satellitism.Jpeg'\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "widget = BBoxWidget(classes=OBJECTS)\n",
        "widget.image = encode_image_pillow(image)\n",
        "widget"
      ],
      "metadata": {
        "id": "1QhJ70lgcrB9"
      },
      "id": "1QhJ70lgcrB9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xyxy_positive = get_normalized_boxes(widget.bboxes, \"positive\", image.size)\n",
        "xyxy_negatives = get_normalized_boxes(widget.bboxes, \"negative\", image.size)\n",
        "\n",
        "inference_state = processor.set_image(image)\n",
        "processor.reset_all_prompts(inference_state)\n",
        "\n",
        "for box in xyxy_positive:\n",
        "    inference_state = processor.add_geometric_prompt(state=inference_state, box=box, label=True)\n",
        "for box in xyxy_negatives:\n",
        "    inference_state = processor.add_geometric_prompt(state=inference_state, box=box, label=False)\n",
        "\n",
        "detections = from_sam(sam_result=inference_state)\n",
        "detections = detections[detections.confidence > 0.5]\n",
        "annotate(image, detections)"
      ],
      "metadata": {
        "id": "vnj-sfpsqzQm"
      },
      "id": "vnj-sfpsqzQm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o6gkB7hwLrr4"
      },
      "id": "o6gkB7hwLrr4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Eha5eaLRLroU"
      },
      "id": "Eha5eaLRLroU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3d8tPluWLrhm"
      },
      "id": "3d8tPluWLrhm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "ff722971-ca5d-431f-8450-ccfea4ff0708",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "oMs6cjnYdyfn"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}